from glob import glob 
import os
import re
import faiss
import numpy as np
from langchain.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.document_loaders import PyPDFium2Loader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
import pickle
from dotenv import load_dotenv
from typing import Optional, Any
from PyPDF2 import PdfReader

from .embedded import *
from modules.extractors import TableExtractor, PdfPlumberExtractor, AzureExtractor
from modules.extractors.azure_extractor import convert_pdf_page_to_image
from modules.utils import save_extracted_text, extract_images_from_pdf
from modules.extractors.image_extractor import (
    crop_figures_from_image, 
    is_graph_image, 
    get_cropped_image_paths, 
    text_description_image
    )


load_dotenv()

endpoint = os.getenv("AZURE_DOCUMENT_ENDPOINT")
key = os.getenv("AZURE_DOCUMENT_KEY")

class Store:
    # PDFãƒ•ã‚¡ã‚¤ãƒ«ã¨ä¼šç¤¾åã®å¯¾å¿œã‚’è¨­å®š
    pdf_to_company = {
        "1.pdf": "4â„ƒ",
        "2.pdf": "IHI",
        "3.pdf": "NISSAN",
        "4.pdf": "KAGOME",
        "5.pdf": "KITZ",
        "6.pdf": "KUREHA",
        "7.pdf": "GLORY",
        "8.pdf": "ã‚µãƒ³ãƒˆãƒªãƒ¼",
        "9.pdf": "ãƒã‚¦ã‚¹é£Ÿå“",
        "10.pdf": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
        "11.pdf": "Media Do",
        "12.pdf": "MOS",
        "13.pdf": "ãƒ©ã‚¤ãƒ•ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³",
        "14.pdf": "é«˜æ¾ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³",
        "15.pdf": "å…¨å›½ä¿è¨¼æ ªå¼ä¼šç¤¾",
        "16.pdf": "æ±æ€¥ä¸å‹•ç”£",
        "17.pdf": "TOYO",
        "18.pdf": "æ—¥æ¸…é£Ÿå“",
        "19.pdf": "meiji"
    }

    def __init__(
        self, 
        client: Any,
        embeddings: Embeddings,
        embedded_path: str,
        faiss_index_path: str,
        extractor_type: str, # æŠ½å‡ºã‚¿ã‚¤ãƒ—ã®è¨­å®š(è©³ç´°ã¯config)
        chunk_size: int = 2000,
        chunk_overlap: int = 200,
        merge_image_text: bool = True,
        text_extractor_type: str = "pdfplumber", # ã‚«ã‚¹ã‚¿ãƒ æŠ½å‡ºæ™‚ã®è¡¨æŠ½å‡ºæ©Ÿæ§‹ã®ã‚¿ã‚¤ãƒ—é¸æŠ
        paipline_config: Optional[dict[str]] = None, # ã‚«ã‚¹ã‚¿ãƒ æŠ½å‡ºæ™‚ã®ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š
    ) -> None:
        self.embeddings = embeddings
        self._pdf_files = glob('data/documents/*.pdf')
        self._documents = []
        self._embedded = []
        self._embedded_path = embedded_path
        self._faiss_index_path = faiss_index_path
        self._chunk_size = chunk_size
        self._chunk_overlap = chunk_overlap
        self._images = {}
        self._client = client
        self._merge_image_text = merge_image_text

        if extractor_type == "azure":
            self.azure_extractor = AzureExtractor(endpoint, key)
            self._azure_create_document()
        elif extractor_type == "extracted":
            self._extracted_create_document()
        elif extractor_type == "custom":
            self.table_extractor = self._select_table_instance(text_extractor_type, client)
            print(paipline_config)
            self._create_metadata(paipline_config)
        self._split_docs = self._split_documents(self._documents)


    def _azure_create_document(self) -> None:
        """
        PDFã‚’èª­ã¿è¾¼ã¿ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ (æ±ç”¨åŒ–)
        """
        for pdf_path in self._pdf_files:
            file_name = os.path.basename(pdf_path)
            company_name = self.pdf_to_company.get(file_name, "ä¸æ˜ãªä¼šç¤¾")  

            reader = PdfReader(pdf_path)
            num_pages = len(reader.pages)
            print(f"{file_name} ã¯ {num_pages} ãƒšãƒ¼ã‚¸ã‚ã‚Šã¾ã™ã€‚")

            for i in range(num_pages):
                page_number = i + 1
                temp_image_path = convert_pdf_page_to_image(pdf_path, i + 1)
                
                # Azure ã®æŠ½å‡ºæ©Ÿæ§‹ã‚’åˆ©ç”¨ã—ã¦ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                extraction_result = self.azure_extractor.extract_text_from_local(temp_image_path)
                markdown_text = extraction_result.get("markdown", "")

                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹
                save_extracted_text(pdf_path, page_number, markdown_text)

                # åº§æ¨™ä½ç½®ã«åŸºã¥ã„ã¦å›³ã‚„ç”»åƒã®ã‚¯ãƒ­ãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã†
                cropped_files = crop_figures_from_image(temp_image_path, extraction_result["figure_positions"])

                # ã‚¯ãƒ­ãƒƒãƒ—ã—ãŸç”»åƒã§ã‚°ãƒ©ãƒ•ä»¥å¤–ã‚’æ’é™¤
                graph_files = is_graph_image(cropped_files) if cropped_files else []

                # ã‚°ãƒ©ãƒ•ç”»åƒã‚’LLMã§ãƒ†ã‚­ã‚¹ãƒˆåŒ–
                image_texts = []
                if graph_files:
                    for graph_file in graph_files:
                        image_text = text_description_image(graph_file, self._client)
                        image_texts.append(image_text)

                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                    merged_image_text = "\n\n".join(image_texts)
                    save_extracted_text(pdf_path, page_number, merged_image_text, sub_dir="image_text")

                # ãƒšãƒ¼ã‚¸ç•ªå·ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸ã—ã¦ Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                metadata = {
                    "page": i + 1,
                    "source_file": file_name,
                    "image_path": temp_image_path,
                    "company": company_name,
                    "grapf_files": graph_files,
                }
                doc = Document(page_content=markdown_text, metadata=metadata)
                self._documents.append(doc)

                # ç”»åƒèª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆæ–¹æ³•ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã§å¤‰æ›´
                if self._merge_image_text:
                    # ç”»åƒã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«çµåˆ
                    merged_text = markdown_text + "\n\n" + "\n".join(image_texts) if image_texts else markdown_text
                    doc = Document(page_content=merged_text, metadata=metadata)
                    self._documents.append(doc)
                else:
                    # ç”»åƒã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ ¼ç´
                    doc_main = Document(page_content=markdown_text, metadata=metadata)
                    self._documents.append(doc_main)

                    for image_text, graph_file in zip(image_texts, graph_files):
                        image_metadata = {
                            "page": i + 1,
                            "source_file": file_name,
                            "company": company_name,
                            "image_path": graph_file, 
                        }
                        doc_image = Document(page_content=image_text, metadata=image_metadata)
                        self._documents.append(doc_image)
    

    def _extracted_create_document(
            self,
            use_image: bool = True, 
            extracted_image_text: bool = True,
    )-> None:
        """
        PDFã”ã¨ã«æŠ½å‡ºæ¸ˆã¿ã®Markdownãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã€
        Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã—ã¦ self._documents ã«è¿½åŠ ã™ã‚‹ã€‚
        """
        for pdf_path in self._pdf_files:
            file_name = os.path.basename(pdf_path)
            base_name = os.path.splitext(file_name)[0]
            company_name = self.pdf_to_company.get(file_name, "ä¸æ˜ãªä¼šç¤¾")  
            graph_files = []

            reader = PdfReader(pdf_path)
            num_pages = len(reader.pages)
            print(f"{file_name} ã¯ {num_pages} ãƒšãƒ¼ã‚¸ã‚ã‚Šã¾ã™ã€‚")

            # PDFã”ã¨ã®æŠ½å‡ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä½œæˆ
            base_dir = os.path.join("data", "extractored_txt", base_name)

            for i in range(num_pages):
                page_number = i + 1

                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã¯ "page_<ãƒšãƒ¼ã‚¸ç•ªå·>.txt" ã¨ãªã£ã¦ã„ã‚‹ã¨ã™ã‚‹
                read_text_path = os.path.join(base_dir, f"page_{page_number}.txt")
                if not os.path.exists(read_text_path):
                    raise FileNotFoundError(f"[ERROR] {read_text_path} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                with open(read_text_path, "r", encoding="utf-8") as f:
                    markdown_text = f.read()  
                
                graph_files = []
                image_texts = []

                # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åŸ‹ã‚è¾¼ã‚€ã‹
                if use_image:
                    if extracted_image_text:
                        image_base_dir = os.path.join("data", "image_text", base_name)
                        read_image_text_path = os.path.join(image_base_dir, f"page_{page_number}.txt")
                        if os.path.exists(read_image_text_path): 
                            with open(read_image_text_path, "r", encoding="utf-8") as f:
                                existing_image_text = f.read().strip()
                                if existing_image_text:
                                    image_texts.append(existing_image_text)
                        else:
                            pass
                    else:
                        # ã‚¯ãƒ­ãƒƒãƒ—æ¸ˆã¿ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã™ã‚‹
                        cropped_files = get_cropped_image_paths(base_name, page_number)

                        # ã‚¯ãƒ­ãƒƒãƒ—ã—ãŸç”»åƒã§ã‚°ãƒ©ãƒ•ä»¥å¤–ã‚’æ’é™¤
                        if cropped_files:
                            graph_files = is_graph_image(cropped_files)

                            # LLM ã§ç”»åƒã‚’èª¬æ˜ã™ã‚‹
                            if graph_files:
                                for graph_file in graph_files:
                                    image_text = text_description_image(graph_file, self._client)
                                    image_texts.append(image_text)
                                # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                                merged_image_text = "\n\n".join(image_texts)
                                save_extracted_text(pdf_path, page_number, merged_image_text, sub_dir="image_text")
                        else:
                            print("ã‚¯ãƒ­ãƒƒãƒ—ã•ã‚ŒãŸç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚")    

                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
                metadata = {
                    "page": page_number,
                    "source_file": file_name,
                    "company": company_name,
                    "graph_files": graph_files,
                }

                # ç”»åƒèª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã®çµ±åˆæ–¹æ³•ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã§å¤‰æ›´
                if self._merge_image_text:
                    # ç”»åƒã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«çµåˆ
                    merged_text = markdown_text + "\n\n" + "\n".join(image_texts) if image_texts else markdown_text
                    doc = Document(page_content=merged_text, metadata=metadata)
                    self._documents.append(doc)
                else:
                    # ç”»åƒã®èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¥ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ ¼ç´
                    doc_main = Document(page_content=markdown_text, metadata=metadata)
                    self._documents.append(doc_main)

                    for image_text, graph_file in zip(image_texts, graph_files):
                        image_metadata = {
                            "page": page_number,
                            "source_file": file_name,
                            "company": company_name,
                            "image_path": graph_file, 
                        }
                        doc_image = Document(page_content=image_text, metadata=image_metadata)
                        self._documents.append(doc_image)


    def _create_metadata(self, config: dict = None) -> None:
        """
        PDFã‚’èª­ã¿è¾¼ã¿ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹ (æ±ç”¨åŒ–)
        
        Args:
            config (dict): å‡¦ç†ã®ON/OFFã‚’è¨­å®šã™ã‚‹è¾æ›¸
                - "include_tables": è¡¨ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆã™ã‚‹ã‹ (True/False)
                - "linearize_tables": è¡¨ã®ç·šå½¢åŒ–ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜ã™ã‚‹ã‹ (True/False)
                - "summarize_tables": LLMã§è¡¨ã‚’è¦ç´„ã—ã¦çµåˆã™ã‚‹ã‹ (True/False)
                - "caption_images": ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆã™ã‚‹ã‹ (True/False)
                - "store_image_captions": ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜ã™ã‚‹ã‹ (True/False)
        """
        if config is None:
            config = {
                "include_tables": True,
                "linearize_tables": False,
                "summarize_tables": False,
                "caption_images": False,
                "store_image_captions": True
            }

        for pdf_path in self._pdf_files:
            file_name = os.path.basename(pdf_path)
            company_name = self.pdf_to_company.get(file_name, "ä¸æ˜ãªä¼šç¤¾")  

            # **ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—**
            loader = PyPDFium2Loader(pdf_path)
            raw_docs = loader.load()

            # **è¡¨ã®å–å¾—**
            tables = self.table_extractor.extract_tables(pdf_path)

            # # **ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®å–å¾— (ãƒšãƒ¼ã‚¸å˜ä½)**
            # image_paths = extract_images_from_pdf(pdf_path, file_name)
            # images = self._process_image(image_paths)
            images = None

            for doc in raw_docs:
                processed_doc = self._process_document(doc, company_name, file_name, config, tables, images)
                self._documents.append(processed_doc)

            print(f'{pdf_path} ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºçµ‚äº†.')


    def _preprocess_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†"""
        text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        return text
    

    def _process_document(self, doc: Document, company_name: str, file_name: str, config: dict, tables: dict, images: dict) -> Document:
        """PDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‰å‡¦ç†ã‚’è¡Œã„ã€Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        page_num = doc.metadata.get("page")
        processed_content = self._preprocess_text(doc.page_content)

        # **è©²å½“ãƒšãƒ¼ã‚¸ã®è¡¨ã‚’çµåˆ**
        if config["include_tables"] and page_num in tables:
            table_list = tables[(page_num)]  # ãƒšãƒ¼ã‚¸å†…ã®ã™ã¹ã¦ã®è¡¨
            print(f"ğŸ“„ å‡¦ç†ä¸­: {file_name} | ãƒšãƒ¼ã‚¸ç•ªå·: {page_num}")

            table_summaries = []  # å„è¡¨ã”ã¨ã®è¦ç´„ãƒªã‚¹ãƒˆ

            for idx, table_data in enumerate(table_list):
                if config["summarize_tables"]:
                    table_summary = self.table_extractor.summarize_table(table_data)
                    print(table_summary)
                    print("")
                    table_summaries.append(table_summary)
                else:
                    formatted_table = "\n".join([" | ".join([str(cell) if cell is not None else "" for cell in row]) for row in table_data if row])
                    table_summaries.append(formatted_table)

            processed_content += "\n\n" + "\n\n".join(table_summaries)

        # **ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ**
        metadata = {
            "company": company_name,
            "source_file": file_name,
            **doc.metadata
        }

        # **è¡¨ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜**
        if config["linearize_tables"] and (file_name, page_num) in tables:
            metadata["linearized_table"] = tables[(file_name, page_num)]

        # **ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è¿½åŠ **
        if config["store_image_captions"] and (file_name, page_num) in images:
            metadata["image_captions"] = images[(file_name, page_num)]

        return Document(page_content=processed_content, metadata=metadata)
        
    
    def _split_documents(self, documents) -> list[Document]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self._chunk_size,
            chunk_overlap=self._chunk_overlap
        )
        return text_splitter.split_documents(documents)
    

    def _select_table_instance(self, text_extractor_type: str, client: Any) -> TableExtractor:
        match text_extractor_type.lower():
            case "pdfplumber":
                return PdfPlumberExtractor(client)
            # case "camelot":
            #     return CamelotExtractor()
            # case "pdf2docx":
            #     return Pdf2DocxExtractor()
            case _:
                raise ValueError(f"æœªçŸ¥ã®è¡¨æŠ½å‡ºæ–¹æ³•: {text_extractor_type}")


    def store_embeddings(self, file_name: str) -> None:
        """åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆã—ã€ä¿å­˜"""
        os.makedirs(self._embedded_path, exist_ok=True)

        self._embedded = self.embeddings.embed_documents([doc.page_content for doc in self._split_docs])

        save_path = os.path.join(self._embedded_path, file_name)

        with open(save_path, "wb") as f:
            pickle.dump(self._embedded, f)

        print(f"å‡¦ç†ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(self._split_docs)}")
        print(f"ç”Ÿæˆã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿æ•°: {len(self._embedded)}")
        print("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")


    def load_embeddings(self, file_name: str) -> None:
        load_path = os.path.join(self._embedded_path, file_name)
        """ä¿å­˜æ¸ˆã¿ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
        if os.path.exists(load_path):
            with open(load_path, "rb") as f:
                self._embedded = pickle.load(f)
            print("ä¿å­˜æ¸ˆã¿ã®åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        else:
            print("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚store_embeddings() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


    @property
    def embedded(self) -> list[list[float]]:
        """åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not self._embedded:
            raise ValueError("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã¾ãŸã¯ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚store_embeddings() ã¾ãŸã¯ load_embeddings() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return self._embedded
    

    def old_build_faiss_index(self) -> None:
        """FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜"""
        if not self._embedded:
            raise ValueError("åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚store_embeddings() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        
        dimension = len(self._embedded[0])  # ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°
        faiss_index = faiss.IndexFlatL2(dimension)
        faiss_index.add(np.array(self._embedded).astype("float32"))

        # Document ã‚’ ID ã§ç®¡ç†
        docstore_data = {str(i): doc for i, doc in enumerate(self._split_docs)}
        docstore = InMemoryDocstore(docstore_data)

        index_to_docstore_id = {i: str(i) for i in range(len(self._split_docs))}


        def embedding_function(texts: list[str]) -> list[list[float]]:
            """è¤‡æ•°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ Azure OpenAI ã§åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›"""
            return self.embeddings.embed_documents(texts)

        self._vectorstore = FAISS(
            index=faiss_index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
            embedding_function=embedding_function
        )

        os.makedirs("data/processed/old_faiss_index", exist_ok=True)

        self._vectorstore.save_local("data/processed/old_faiss_index")
        print("FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãƒ»ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


    def build_faiss_index(self) -> None:
        """FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜"""
        self._vectorstore = FAISS.from_documents(
            documents=self._split_docs,  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
            embedding=self.embeddings  # äº‹å‰ã«ä½œæˆã—ãŸåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
        )

        os.makedirs(self._faiss_index_path, exist_ok=True)

        self._vectorstore.save_local(self._faiss_index_path)
        print("FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆãƒ»ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    @property
    def vectorstore(self) -> FAISS:
        return self._vectorstore
        
